# ğŸ“Š Complete TensorBoard Guide for Reinforcement Learning

*A comprehensive reference for understanding and interpreting TensorBoard metrics in any RL project*

---

## ğŸ¯ Table of Contents

1. [Performance Metrics](#-performance-metrics)
2. [Policy Learning Metrics](#-policy-learning-metrics)  
3. [Value Function Metrics](#-value-function-metrics)
4. [Exploration vs Exploitation](#-exploration-vs-exploitation)
5. [Training Stability Metrics](#-training-stability-metrics)
6. [Algorithm-Specific Metrics](#-algorithm-specific-metrics)
7. [Debugging Guide](#-debugging-guide)
8. [Best Practices](#-best-practices)

---

## ğŸ† Performance Metrics

### **eval/mean_reward** (Most Important!)
**What it measures:** Average total reward per episode during evaluation
**Range:** Depends on your environment (-âˆ to +âˆ)
**Good trends:**
- âœ… Steady upward trend
- âœ… Reaches and maintains high values
- âœ… Low variance between episodes

**Bad trends:**
- âŒ Flat or decreasing
- âŒ High variance/instability
- âŒ Sudden drops after good performance

**How to use:**
- Primary indicator of learning success
- Use to decide when to stop training
- Compare different hyperparameters
- Set reward thresholds for early stopping

**Debugging:**
- If flat: Learning rate too low, or environment too hard
- If decreasing: Overfitting, learning rate too high
- If noisy: Increase evaluation episodes, check environment randomness

### **eval/mean_ep_length**
**What it measures:** Average number of steps per episode during evaluation
**Range:** 1 to max_episode_steps
**Good trends:**
- âœ… Increasing towards maximum (for survival tasks)
- âœ… Stable at expected length (for fixed-length tasks)
- âœ… Low variance

**Bad trends:**
- âŒ Decreasing over time (agent getting worse)
- âŒ High variance
- âŒ Stuck at very low values

**How to use:**
- Indicates if agent is failing early
- Shows task completion ability
- Helps identify if reward shaping is needed

### **train/mean_reward** 
**What it measures:** Average reward during training (often noisier than eval)
**Interpretation:** Similar to eval/mean_reward but with more variance
**Use case:** Real-time training progress monitoring

---

## ğŸ§  Policy Learning Metrics

### **train/policy_gradient_loss** (PPO/A3C/etc.)
**What it measures:** How much the policy is changing each update
**Range:** Usually negative values
**Good trends:**
- âœ… Decreasing magnitude (getting less negative)
- âœ… Smooth curve without large spikes
- âœ… Converges to small values

**Bad trends:**
- âŒ Large oscillations
- âŒ Continuously increasing magnitude
- âŒ Sudden spikes

**How to use:**
- Monitor learning progress
- Detect training instability
- Tune learning rate (high loss â†’ lower LR)

**Algorithm notes:**
- PPO: Should be negative and decreasing
- TRPO: Similar behavior expected
- DQN: Uses different loss function (see Q-learning section)

### **train/approx_kl** (PPO/TRPO)
**What it measures:** KL divergence between old and new policies
**Range:** 0 to +âˆ (typically 0.001 to 0.1)
**Target range:** Usually 0.01 to 0.05
**Good trends:**
- âœ… Small, stable values
- âœ… Occasional small spikes (normal)
- âœ… Not growing over time

**Bad trends:**
- âŒ Consistently high values (>0.1)
- âŒ Growing trend
- âŒ Large frequent spikes

**How to use:**
- Ensure policy updates aren't too aggressive
- Tune learning rate and PPO clip range
- Early stopping if KL gets too high

### **train/clip_fraction** (PPO)
**What it measures:** Fraction of policy updates that got clipped
**Range:** 0.0 to 1.0
**Target range:** 0.1 to 0.3 for healthy learning
**Good trends:**
- âœ… Moderate values (0.1-0.3)
- âœ… Gradually decreasing as learning stabilizes
- âœ… Stable during good performance

**Bad trends:**
- âŒ Always near 0 (updates too conservative)
- âŒ Always near 1 (updates too aggressive)
- âŒ Highly variable

**How to use:**
- Tune PPO clip range (default 0.2)
- Adjust learning rate
- Monitor training aggressiveness

---

## ğŸ’° Value Function Metrics

### **train/value_loss**
**What it measures:** Error in predicting future rewards
**Range:** 0 to +âˆ (lower is better)
**Good trends:**
- âœ… Decreasing over time
- âœ… Converging to low, stable value
- âœ… Smooth curve

**Bad trends:**
- âŒ Increasing over time
- âŒ Large oscillations
- âŒ Never converging

**How to use:**
- Monitor critic network learning
- Tune value function learning rate
- Detect overfitting in value estimation

**Debugging:**
- High loss: Value LR too high, or target too hard to predict
- Increasing loss: Overfitting, need regularization
- Oscillating: Unstable training, reduce learning rate

### **train/explained_variance**
**What it measures:** How well value function explains reward variance
**Range:** 0.0 to 1.0 (1.0 is perfect prediction)
**Good values:** > 0.5 for most tasks
**Good trends:**
- âœ… Increasing towards 1.0
- âœ… Stable at high values (>0.7)
- âœ… Smooth improvement

**Bad trends:**
- âŒ Decreasing over time
- âŒ Stuck at very low values (<0.1)
- âŒ Highly variable

**How to use:**
- Assess value function quality
- Debug reward prediction issues
- Tune value network architecture

### **train/value_function_error**
**What it measures:** Mean absolute error of value predictions
**Interpretation:** Similar to value_loss but in original reward units
**Use case:** More interpretable than loss functions

---

## ğŸ² Exploration vs Exploitation

### **train/entropy_loss** 
**What it measures:** Randomness in policy decisions
**Range:** Negative values (more negative = more random)
**Good trends:**
- âœ… High entropy early (exploration)
- âœ… Gradually decreasing (focusing on good actions)
- âœ… Stabilizes at moderate level

**Bad trends:**
- âŒ Drops to zero too quickly (premature convergence)
- âŒ Stays too high (never learns to exploit)
- âŒ Highly variable

**Algorithm differences:**
- **PPO/A3C:** Entropy bonus encourages exploration
- **DQN:** Uses epsilon-greedy instead
- **SAC:** Automatically balances exploration

**How to tune:**
- Increase entropy coefficient for more exploration
- Decrease if agent never exploits good strategies
- Use entropy scheduling (high â†’ low over training)

### **train/epsilon** (DQN family)
**What it measures:** Probability of random action selection
**Range:** 0.0 to 1.0
**Typical schedule:** 1.0 â†’ 0.1 over training
**Good trends:**
- âœ… Starts high (1.0 or 0.9)
- âœ… Gradually decreases
- âœ… Stabilizes at low value (0.01-0.1)

**How to use:**
- Balance exploration vs exploitation
- Tune epsilon decay schedule
- Monitor if agent explores enough

---

## âš™ï¸ Training Stability Metrics

### **train/learning_rate**
**What it measures:** Current learning rate (if using scheduling)
**Common schedules:**
- Constant (flat line)
- Linear decay
- Exponential decay
- Cosine annealing

**How to use:**
- Monitor LR scheduling
- Tune initial learning rate
- Implement adaptive scheduling

### **train/clip_range** (PPO)
**What it measures:** Current PPO clipping parameter
**Typical values:** 0.1 to 0.3
**Usage:** Can be scheduled like learning rate

### **train/loss** (Total Loss)
**What it measures:** Combined loss from all components
**Components:** Policy loss + Value loss + Entropy loss
**Good trends:**
- âœ… Generally decreasing
- âœ… Stabilizes during good performance
- âœ… Smooth curve

### **train/grad_norm**
**What it measures:** Magnitude of gradients during training
**Range:** 0 to +âˆ
**Good values:** Usually 0.1 to 10.0
**Problems:**
- Very high (>100): Gradient explosion
- Very low (<0.001): Vanishing gradients
- Growing over time: Training instability

---

## ğŸ”„ Algorithm-Specific Metrics

### **PPO (Proximal Policy Optimization)**
**Key metrics:**
- `policy_gradient_loss`: Policy improvement signal
- `value_loss`: Critic learning progress  
- `approx_kl`: Policy change magnitude
- `clip_fraction`: Update aggressiveness
- `entropy_loss`: Exploration level

**Healthy PPO training:**
- Policy loss: Decreasing and negative
- Value loss: Decreasing to stable level
- KL divergence: 0.01-0.05 range
- Clip fraction: 0.1-0.3 range

### **DQN (Deep Q-Network)**
**Key metrics:**
- `q_loss` or `td_error`: Temporal difference error
- `mean_q_value`: Average predicted Q-values
- `epsilon`: Exploration probability
- `target_network_update`: When target updates

**Healthy DQN training:**
- Q-loss: Decreasing over time
- Mean Q-value: Should increase as agent improves
- Epsilon: Gradual decrease from 1.0 to ~0.1

### **SAC (Soft Actor-Critic)**
**Key metrics:**
- `actor_loss`: Policy improvement
- `critic_loss`: Value function learning
- `alpha_loss`: Temperature parameter learning
- `entropy`: Policy entropy (automatic tuning)

### **A3C/A2C (Actor-Critic)**
**Key metrics:**
- `policy_loss`: Actor network loss
- `value_loss`: Critic network loss
- `entropy`: Policy randomness
- `advantage`: Advantage function values

---

## ğŸ”§ Debugging Guide

### **Problem: Reward Not Improving**

**Symptoms:**
- Flat eval/mean_reward
- High variance in performance
- Policy loss not decreasing

**Potential causes & solutions:**
1. **Learning rate too low** â†’ Increase LR
2. **Environment too hard** â†’ Simplify task or add reward shaping
3. **Poor exploration** â†’ Increase entropy coefficient or epsilon
4. **Network too small** â†’ Increase network size
5. **Bad hyperparameters** â†’ Grid search key parameters

### **Problem: Training Unstable**

**Symptoms:**
- Large oscillations in losses
- Performance keeps dropping
- High KL divergence or gradient norms

**Solutions:**
1. **Reduce learning rate** (most common fix)
2. **Increase batch size**
3. **Add gradient clipping**
4. **Reduce PPO clip range**
5. **Check environment for bugs**

### **Problem: Overfitting**

**Symptoms:**
- Training reward >> Evaluation reward
- Performance degrades after peak
- Value loss increasing while policy loss decreasing

**Solutions:**
1. **Early stopping** based on eval performance
2. **Regularization** (dropout, weight decay)
3. **Reduce network size**
4. **More diverse training environments**
5. **Shorter training episodes**

### **Problem: Premature Convergence**

**Symptoms:**
- Entropy drops to zero quickly
- Policy becomes deterministic too early
- Suboptimal final performance

**Solutions:**
1. **Increase entropy coefficient**
2. **Slower entropy decay**
3. **Higher exploration (epsilon/temperature)**
4. **Curriculum learning**

---

## ğŸ“ˆ Best Practices

### **Monitoring Strategy**
1. **Primary metrics:** eval/mean_reward, eval/mean_ep_length
2. **Secondary metrics:** policy_loss, value_loss, entropy
3. **Debug metrics:** approx_kl, clip_fraction, grad_norm
4. **Update frequency:** Every 1000-10000 steps for evaluation

### **Hyperparameter Tuning Priority**
1. **Learning rate** (most important)
2. **Network architecture**
3. **Batch size / n_steps**
4. **Entropy coefficient**
5. **Discount factor (gamma)**

### **When to Stop Training**
**Stop when:**
- âœ… Evaluation reward plateaus at satisfactory level
- âœ… Performance variance becomes very low
- âœ… Value loss starts increasing while policy plateaus

**Don't stop due to:**
- âŒ Training reward plateaus (check eval instead)
- âŒ Temporary performance drops
- âŒ High value loss if eval performance is good

### **Experiment Tracking**
```python
# Log custom metrics
writer.add_scalar('custom/success_rate', success_rate, step)
writer.add_scalar('custom/episode_length_std', length_std, step)
writer.add_scalar('custom/reward_per_step', reward/length, step)
```

### **TensorBoard Organization**
```
logs/
â”œâ”€â”€ experiment_1_baseline/
â”œâ”€â”€ experiment_2_higher_lr/
â”œâ”€â”€ experiment_3_larger_network/
â””â”€â”€ experiment_4_different_reward/
```

---

## ğŸ¯ Quick Reference Cheat Sheet

| Metric | Good Range | Trend | Red Flags |
|--------|------------|-------|-----------|
| eval/mean_reward | Task-dependent | â†—ï¸ Increasing | â†˜ï¸ Decreasing after peak |
| eval/mean_ep_length | Near maximum | â†—ï¸ or â†’ Stable | â†˜ï¸ Decreasing |
| policy_gradient_loss | Negative, small | â†˜ï¸ Decreasing magnitude | Large oscillations |
| value_loss | Low, positive | â†˜ï¸ Decreasing | â†—ï¸ Increasing |
| approx_kl (PPO) | 0.01 - 0.05 | â†’ Stable | > 0.1 consistently |
| clip_fraction (PPO) | 0.1 - 0.3 | â†˜ï¸ Gradually decreasing | Always 0 or 1 |
| entropy_loss | Moderate negative | â†˜ï¸ Gradual decrease | Drops to 0 quickly |
| explained_variance | > 0.5 | â†—ï¸ Increasing | < 0.1 or decreasing |

---

## ğŸ“š Additional Resources

- **Stable Baselines3 Docs:** [https://stable-baselines3.readthedocs.io/](https://stable-baselines3.readthedocs.io/)
- **OpenAI Spinning Up:** [https://spinningup.openai.com/](https://spinningup.openai.com/)
- **TensorBoard Guide:** [https://www.tensorflow.org/tensorboard](https://www.tensorflow.org/tensorboard)

---

*This guide covers the most common RL algorithms and metrics. For specialized algorithms or custom environments, additional metrics may be relevant.*
